---
title: "Series02-RStudio: Correlations and statistical tests"
author: "Joël SINGER"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: false
    number_sections: true
---

# Correlations and linear regression
In this series of exercises, you will explore correlations and linear regression, using a dataset from a real experiment.


## The problem
You want to know if there is a relationship between the nonuse of the proximal part of the upper limb (PANU) and the nonuse of the shoulder (SANU) or the elbow (EENU). 


## The data
As you will see, the data is far from perfect, but this is the reality of experimental data, especially in the case of clinical data.

Download the file NonUse.csv from the course Moodle.

```{r load-data, message=FALSE, warning=FALSE}
nonuse <- read.csv("data/test_data/NonUse.csv", sep = ",", header=TRUE)
head(nonuse)
summary(nonuse)
```


This file contains measures of upper extremity nonuse:

- PANU: Proximal Arm Non Use
- SANU: Shoulder Antepulsion Non Use
- EENU: Elbow Extension Non Use

In your notebook, make a clear description of the experiment, the variables and the data. Do not forget to give the units of each variable, and the range of possible values.\

  | name | description | unit | range |
  |------|-------------|------|-------|
  | PANU | Proximal Arm Non Use | % | 0-100 |
  | SANU | Shoulder Antepulsion Non Use | % | 0-100 |
  | EENU | Elbow Extension Non Use | % | 0-100 |


## Correlation analysis
Below are some questions to guide your analysis.

- Is PANU correlated with SANU and/or EENU?

We use a Spearman correlation, because we do not have assumptions about data distribution, and we do not assume a linear relationship between the variables.
```{r correlation-tests}
cor.test(nonuse$PANU, nonuse$SANU, method = "spearman")
cor.test(nonuse$PANU, nonuse$EENU, method = "spearman")
```


- Visualize the PANU-SANU relationship (and the PANU-EENU relationship)

```{r plot-correlations}
library(ggplot2)
p1 <- ggplot(nonuse, aes(x = PANU, y = SANU)) +
  geom_point() +
  xlim(-100, 100) +
  ylim(-100, 100) +
  labs(title = "PANU vs SANU", subtitle = "Whole possible range of data", x = "PANU (%)", y = "SANU (%)") +
  theme_minimal()

p2 <- ggplot(nonuse, aes(x = PANU, y = EENU)) +
  geom_point() +
  xlim(-100, 100) +
  ylim(-100, 100) +
  labs(title = "PANU vs EENU", subtitle = "Whole possible range of data", x = "PANU (%)", y = "EENU (%)") +
  theme_minimal()

show(p1)
show(p2)
```


- Perform a linear regression for PANU-SANU (and for PANU-EENU)

We do not predict a linear regression, but let us see it nonetheless.

```{r panu-sanu_linear-regression}
library(ggplot2)
p1 <- ggplot(nonuse, aes(x = PANU, y = SANU)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  # xlim(-100, 100) +
  # ylim(-100, 100) +
  labs(title = "PANU vs SANU", x = "PANU (%)", y = "SANU (%)") +
  theme_minimal()

show(p1)
```
```{r panu-eenu_linear-regression}
library(ggplot2)
p1 <- ggplot(nonuse, aes(x = PANU, y = EENU)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  # xlim(-100, 100) +
  # ylim(-100, 100) +
  labs(title = "PANU vs EENU", x = "PANU (%)", y = "EENU (%)") +
  theme_minimal()

show(p1)
```


- Give the regression equations: PANU = a * SANU + b and PANU = a * EENU + b

```{r regression-equations}
model_sanu <- lm(PANU ~ SANU, data = nonuse)
summary(model_sanu)
coefficients_sanu <- coefficients(model_sanu)
a_sanu <- coefficients_sanu["SANU"]
b_sanu <- coefficients_sanu["(Intercept)"]
cat("Regression equation for PANU as a function of SANU: PANU =", a_sanu, "* SANU +", b_sanu, "\n")
model_eenu <- lm(PANU ~ EENU, data = nonuse)
summary(model_eenu)
coefficients_eenu <- coefficients(model_eenu)
a_eenu <- coefficients_eenu["EENU"]
b_eenu <- coefficients_eenu["(Intercept)"]
cat("Regression equation for PANU as a function of EENU: PANU =", a_eenu, "* EENU +", b_eenu, "\n")
```

- Summarize your results as you would do in your master’s thesis report, or as in a scientific article (e.g., What sentence will you write in the article explaining your result?).

PANU is significantly correlated with SANU and EENU according to the Spearman test (p < 0.01), meaning that higher values of SANU and EENU are associated with higher values of PANU. However, when we tested the linear correlation, we found no significant relationship between PANU and SANU (p = 0.17), but a significant (p < 0.001), although poor (adjusted r² = 0.22), linear correlation between PANU and EENU. This suggests that while there is a monotonic relationship between these variables, the EENU seems more reliable to describe PANU.

- What are the limits of your analysis?

This analysis does not propose a combined model of SANU and EENU to describe PANU, which could be more informative. Additionally, the data may contain non-linear relationships that are not captured by the linear regression model, although the data distribution does not suggest it. Furthermore, we did not test for normality or even check visually the variables distribution, which could have given insights about the statistical power of the sample, or the statistical tests to use.




# Statistical tests: comparison of medians (or means?)
In this series of exercises, you will explore how to compare two groups of data, using non-parametric tests (or parametric tests… if it makes sense to do so).


## Make clear what is a statistical test, and when to use it.
Below are some questions to guide your analysis. You can answer them in your notebook, with a short sentence for each question.

- What is a median? A mean?

The median is the value separating both halves of a dataset when it is ordered from lowest to highest, whereas the arithmetic mean is the sum of observations divided by the number of observations. Both metrics relates to the central tendency of the dataset. (1)

- What is a variance? A standard deviation?

Variance expresses the dispersion of data around the mean of a dataset, calculated as the average of the squared differences from the mean. The standard deviation is the square root of the variance, providing a measure of dispersion in the same units as the data. (1)

- What is a normal distribution?

A normal distribution, also known as Gaussian distribution, is a continuous probability distribution characterized by data symmetrically distributed around the mean, with most values clustering around the mean and fewer values appearing as you move away from it. It is defined by its mean and standard deviation. (1)

- What is a statistical test?

Statistical tests are procedures by which we can consensually draw conclusions about a population based on observations from samples. Each test has its null hypothesis (H0) to be challenged against an alternative hypothesis. (1)

- When to use a statistical test?

It is used when we want to explore a research question about metrics in a single or multiple populations for which we already have hypotheses. (1)

- What is a parametric test? A non-parametric test?

Parametric tests are statistical tests for which the data must meet a distributional assumption, whereas non-parametric tests can be applied without making this type of assumptions. (1)

- What are the assumptions of parametric tests?

Parametric tests assume that the data are at least ordinal and follows a specific distribution (usually normal), and, depending on test, that the samples are random and independently chosen from the population or that the variances of the groups being compared are equal (homoscedasticity). (2)

- What are the assumptions of non-parametric tests?

Non-parametric tests assume that the data are at least ordinal and that it does not need to follow a specific distribution, although, depending on the test used, data should be random and independently chosen from the population or homoscedasticity must be met. (2,3)

- What is a p-value?

"The P-value is the probability, calculated assuming that the null hypothesis is true, of obtaining a value of the test statistic at least as contradictory to H0 as the value calculated from the available sample." (1, p.456) In more simple words, it is the probability that the value presented by the test stands even when the null hypothesis is true.

- What is the risk of error when using a statistical test?

When using a statistical test, two types of error can occur: to reject H0 when it is true (type I error, the most used to decide whether to reject null hypothesis), or to not reject H0 when it is false (type II error, often not considered). (1)

- What is the difference between a paired and an unpaired test?

A paired test is used when the samples are matched in some way, whereas an unpaired test is used when the samples are independent of each other. (1)

You can use your favorite LLM, book or web site to answer these questions, but you have to provide at least one reference for each answer (book, article, URL, LLM prompt, etc.).

References:

(1) Devore JL, Berk KN, Carlton MA. Modern Mathematical Statistics with Applications. Cham: Springer International Publishing; 2021. (Springer Texts in Statistics).

(2) Daniel Barch, Tufts University School of Arts and Sciences (Medford, Massachussets), textbook of: Psychology 207 and 208: Advanced Statistics I and II.

(3) List of non-parametric tests: https://en.wikipedia.org/w/index.php?title=Nonparametric_statistics&oldid=1309608445 (29.09.2025).

## Effect of treatment over time
This is a very classical question in clinical or sport research: does a treatment-training have an effect over time?

To answer this question scientifically, measurements must be taken before and after treatment, and possibly later, in order to assess whether the effect is lasting or not.

### The data
Download the file PrePost.csv

This file contains before/after measurements. The treatment is a rehabilitation training for individuals with cardiac conditions.

Before going further, you have to make clear what are the measured variables. 

```{r load-prepost-data}
data <- read.table("data/test_data/PrePost.csv", sep = ",", header=TRUE)
summary(data)
```
Here, we can see that the values in the column "time" are not considered as factors, so we convert them:
```{r convert-time-to-factor}
data$time <- as.factor(data$time)
summary(data)
```

- What do they measure? 

We can see a "perf" variable, from which no assumptions can be made as no information is given about it in the file or in the question.

- What are the units? 

We do not know because we do not know what the variable measures.

- What are the possible values? 

We do not know because we do not know what the variable measures.

- What are the expected changes over rehabilitation.

We do not know because we do not know what the variable measures. However, we could make the hypothesis of an increase in the variable measured, because a "perf increase" would be synonym of improvement for subjects.

- Make a table, with one row per variable, and the following columns:

| Variable | Description | Unit | Possible values | Expected change |

| Variable | Description | Unit | Possible values | Expected change |
|----------|-------------|------|-----------------|-----------------|
|   perf   |      ?      |   ?  |        ?        |     Increase    |

### The analysis
You want to know if the treatment has an effect on one or more of the measured variables.

- Does the treatment have an effect?

We will decide it over the results of our statistical analysis. What is useful in this case is the fact that not knowing what the data is about does not have an influence on the statistical analysis.

- What comparison are you going to make?

We will compare the "perf" variable before and after treatment.

- Which test will you use? Parametric or non-parametric?

We can observe that the sample size is small (n=8), which makes difficult to assess the normality of the distribution. Indeed, the normality test's result would not be relevant for small samples like this (<12 observations) (1).

Thus, we will use a non-parametric test. For the precise test to be used, we can see that the samples are paired (before and after treatment for the same subjects), so we will use the Wilcoxon signed-rank test that is designed for paired samples. We will then compare medians.

```{r wilcoxon-test }
# Create vectors for before and after treatment
perf_pre <- data$perf[data$time == "Before"]
perf_post <- data$perf[data$time == "After"]
# Perform the Wilcoxon signed-rank test
wilcox.test(perf_pre, perf_post, paired = TRUE, alternative = "two.sided")
```

The Wilcoxon signed-rank test shows a p-value of 0.01356, which is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that there is a significant difference in "perf" before and after treatment.\
Concerning the direction of change, although obvious from eyes, we can compare the medians of both conditions:

```{r direction of change}
median_pre <- median(perf_pre)
median_post <- median(perf_post)
cat("Median before treatment:", median_pre, "\n")
cat("Median after treatment:", median_post, "\n")
if (median_post > median_pre) {
  cat("The treatment has led to an increase in 'perf'.\n")
} else if (median_post < median_pre) {
  cat("The treatment has led to a decrease in 'perf'.\n")
} else {
  cat("There is no change in 'perf' after treatment.\n")
}   
```


- Which graph illustrates the effect of the treatment?

To graph the evolution of the group over time, we can use a slopegraph, which is particularly useful for visualizing changes in paired data. Indeed, it makes it easy to see the direction and magnitude of change for each individual.


```{r slopegraph}
library(ggplot2)

# Create an ID for each subject
ID <- rep(1:(nrow(data)/2)) 
data <- cbind(data, ID)


p <- ggplot(data, aes(x = time, y = perf, group = ID)) +
  geom_point() +
  geom_line() +
  scale_x_discrete(limits = c("Before", "After")) +
  labs(title = "Performance Before and After Treatment",
       x = "Time", y = "Performance") +
  theme_minimal()

p +
stat_summary(aes(group = 1), fun = median, geom = "line",
  linewidth = 1.2, colour = "#D55E00") +
stat_summary(aes(group = 1), fun = median, geom = "point",
  size = 3, shape = 21, fill = "white", colour = "#D55E00")

```

We used the median to characterize central tendency, as we used a non-parametric test, but the mean would have been more relevant to visualize the group evolution. Indeed, if the two individuals at the center of the set had not increased their performance, the median would not have changed, whereas the mean would have increased.

*To add medians (thanks to stat_summary()), I used the OpenAI/GPT 5 Mini LLM with the following prompt: "J'utilise RStudio pour créer un slopegraph de type "avant-après". Seulement, je n'arrive pas à intégrer dans le graphe les médianes pour "avant" et "après". Comment pourrais-je faire ?".*

- What sentence will you write in your thesis to explain your result?

The Wilcoxon signed-rank test revealed a significant difference in "perf" before and after treatment (p = 0.01356). When we look at each individual, we can see that all subjects improved their performance after treatment. If the protocol was well controlled, which we do not know in this case given the scarce information, we could conclude that the treatment had a positive effect on "perf".

Reference:

(1) Ghasemi A, Zahedias S. Normality tests for statistical analysis: a guide for non-statisticians. Int J Endocrinol Metab. 2012 Spring;10(2):486-9. doi: 10.5812/ijem.3505. Epub 2012 Apr 20. PMID: 23843808; PMCID: PMC3693611.

## Testing some stereotypes
Humans have stereotypes, some of them are probably true, some others are probably false (e.g., ref ).

Here, we will test stereotypes about snorers… among others.

### The data
Download the file snore.txt

This file contains anthropometric and qualitative measurements (1 person per line).

```{r load-snore-data}
snore <- read.table("data/test_data/snore.txt", sep = "\t", header=TRUE)

# Factor variables
snore$sex <- as.factor(snore$sex)
snore$snore <- as.factor(snore$snore)
snore$tabac <- as.factor(snore$tabac)

summary(snore)
```


### The analysis
Do the data confirm the following stereotypes? Provide a reasoned answer (data, figure, result sentence) for each question.

**- Are snorers fatter?**

The data that is provided does not allow for a precise quantification of fat mass. However, we can use the extensively studied BMI (m/(t^2)) as a proxy for fatness, although it would assume that each individual has the same fat-free mass proportion.\
```{r BMI-calculation}
snore$bmi <- snore$weight/(snore$height/100)^2  # Height is in cm
summary(snore$bmi)
```

Firstly, let us visualize the BMI distribution for both snorers and non-snorers:
```{r bmi-distribution}
library(ggplot2)

ggplot(snore, aes(x = bmi, fill = snore)) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  labs(title = "Density plot of BMI by snoring status",
       x = "BMI", y = "Count") +
  theme_minimal()
```

By visual inspection, we can see that the BMI distributions are quite similar. Let us verify it by a test nonetheless.\
Let us first check the normality of the distributions:
```{r normality-test-bmi}
shapiro.test(snore$bmi[snore$snore == "N"])
shapiro.test(snore$bmi[snore$snore == "O"])
```
The Shapiro-Wilk normality test shows that both distributions significantly deviate from normality (p < 0.001).\
Thus, we will use a non-parametric test to compare the medians of both groups. Here, the median is relevant because BMI is subjected to high variations as we can see in the summary and density plot.\
Two tests are possible: the Mann-Whitney U test or the Brunner Munzel test. The latter is more robust as it does not need for equal variances between tested groups (1). Let us check variances equality with the F-test to choose one:
```{r variance-test-bmi}
var.test(bmi ~ snore, data = snore)
```

Variances cannot be considered different (p=0.92), so we can use the Mann-Whitney U test:
```{r mann-whitney-bmi}
wilcox.test(bmi ~ snore, data = snore, exact = FALSE)
```

The Mann-Whitney U test shows no significant difference in BMI between snorers and non-snorers (p=0.95). Thus, based on this sample and the BMI calculation, the stereotype of snorers being fatter cannot hold.\

Reference:

(1) Karch JD. Psychologists Should Use Brunner-Munzel’s Instead of Mann-Whitney’s U Test as the Default Nonparametric Procedure. Advances in Methods and Practices in Psychological Science. 2021;4(2). doi:10.1177/2515245921999602.
  

**- Do snorers drink or smoke more?**

Let us first consider the smoke stereotype.\
We can visualize the smoking status distribution for both snorers and non-snorers:
```{r smoking-distribution}
library(ggplot2)

ggplot(snore, aes(x = tabac, fill = snore)) +
  geom_bar(position = "dodge", na.rm = TRUE) +
  labs(title = "Smoking status by snoring status",
       x = "Smoking status", y = "Count") +
  theme_minimal()
```

Visually, it seems like smoking does not affect snoring status: occurrences are quite similar for snorers, whether they are smoking or not.\
We are comparing two categorical variables (smoking status and snoring status) from independant groups, so we have the choice between the Chi-squared test and the Fisher's exact test. The latter is more accurate for small samples or when expected frequencies in any cell of the contingency table are below 5. So let us check the expected frequencies:
```{r expected-frequencies-smoking}
table_snore_tabac <- table(snore$snore, snore$tabac)
table_snore_tabac
chisq.test(table_snore_tabac)$expected
```

All expected frequencies are above 5, so we can use the Chi-squared test:
```{r chi-squared-smoking}
chisq.test(table_snore_tabac)
```

The Chi-squared test shows no significant association between snoring status and smoking status (p=0.41). Thus, based on this sample, the stereotype of snorers smoking more is not true.

Now, let us consider the drinking stereotype.\
We can visualize the drinking occurrence (we do not know if it is daily, weekly or mensually) distribution for both snorers and non-snorers:
```{r drinking-distribution}
library(ggplot2)
ggplot(snore, aes(x = alcool, fill = snore)) +
  geom_bar() +
  labs(title = "Drinking occurrence by snoring status",
       x = "Drinking occurrence", y = "Count") +
  theme_minimal()
ggplot(snore, aes(x = alcool, fill = snore)) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  labs(title = "Density plot of drinking occurrence by snoring status",
       x = "Drinking occurrence", y = "Count") +
  theme_minimal()
```

We want to evaluate if snorers drink more. Then, we can consider the "alcool" variable as scalable (as for the BMI stereotype), and we can choose between the Mann-Whitney U and the Brunner Munzel tests to compare the medians of both groups (distributions are obviously not normal).\
Let us check variances equality with the F-test to choose one:
```{r variance-test-drinking}
var.test(alcool ~ snore, data = snore)
```

Variances cannot be considered different (p=0.55), so we can use the Mann-Whitney U test:
```{r mann-whitney-drinking}
wilcox.test(alcool ~ snore, data = snore, exact = FALSE)
```

The distributions are significantly different (p=0.009). Let us check the direction:
```{r mann-whitney-drinking-less}
wilcox.test(alcool ~ snore, data = snore, exact = FALSE, 
            alternative = "less")
```

The Mann-Whitney U test shows that snorers drink significantly less than non-snorers (p=0.004). Thus, based on this sample, the stereotype of snorers drinking more is false, and, in fact, the opposite seems to be true.\
However, groups are quite unbalanced (35 non-snorers vs 65 snorers), which is not ideal to assume the independence of groups. 

**- Are men fatter?**

We can visualize the BMI distribution in function of sex:
```{r bmi-sex-distribution}
library(ggplot2)
ggplot(snore, aes(x = bmi, fill = sex)) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  labs(title = "Density plot of BMI by sex",
       x = "BMI", y = "Count") +
  theme_minimal()
```

By visual inspection, we can see that the BMI distributions are quite similar. Let us verify it by a test nonetheless.\
Let us first check the normality of the distributions:
```{r normality-test-bmi-sex}
shapiro.test(snore$bmi[snore$sex == "F"])
shapiro.test(snore$bmi[snore$sex == "H"])
```

The Shapiro-Wilk normality test shows that both distributions significantly deviate from normality (p < 0.001).\
Thus, we will use a non-parametric test to compare the medians of both groups. Let us check variances equality with the F-test:
```{r variance-test-bmi-sex}
var.test(bmi ~ sex, data = snore)
```

Variances cannot be considered different (p=0.83), so we can use the Mann-Whitney U test:
```{r mann-whitney-bmi-sex}
wilcox.test(bmi ~ sex, data = snore, exact = FALSE)
```

The Mann-Whitney U test shows no significant difference in BMI between men and women (p=0.49). Thus, based on this sample and the BMI calculation, the stereotype of one sex being fatter than the other would not be true.\
This does not align with physiology, as we know that women have a higher fat mass proportion than men (1), but the BMI does not allow to assess it precisely. However, epidemiological studies show that men have a higher prevalence of overweight and obesity than women (1). These phenomena could explain our balanced result.

Reference:

(1) Muscogiuri G, Verde L, Vetrani C, Barrea L, Savastano S, Colao A. Obesity: a gender-view. J Endocrinol Invest. 2024 Feb;47(2):299-306. doi: 10.1007/s40618-023-02196-z.

**- Do women smoke less?**

We can visualize the smoking status distribution for both sexes:
```{r smoking-sex-distribution}
library(ggplot2)
ggplot(snore, aes(x = tabac, fill = sex)) +
  geom_bar(position = "dodge", na.rm = TRUE) +
  labs(title = "Smoking status by sex",
       x = "Smoking status", y = "Count") +
  theme_minimal()
```

Graphically, it seems like men smoke more than women. Let us verify it.\
We check the expected frequencies in the contingency table:
```{r expected-frequencies-smoking-sex}
table_sex_tabac <- table(snore$sex, snore$tabac)
table_sex_tabac
chisq.test(table_sex_tabac)$expected
```
All expected frequencies are above 5, so we can use the Chi-squared test:
```{r chi-squared-smoking-sex}
chisq.test(table_sex_tabac)
```

The Chi-squared test shows a significant association between sex and smoking status (p=0.008). Thus, based on this sample, the stereotype of men smoking more than women seems true.\
However, even than for the drinking stereotype, groups are unbalanced (25 women vs 75 men), which makes them difficult to be objectively compared.

**- From a more general perspective: are there any correlations between variables?**

We can visualize the correlations between all quantitative variables with a pairs plot:
```{r pairs-plot}
pairs(snore[, c("age", "height", "weight", "bmi", "alcool")],
      main = "Pairs plot of quantitative variables")
```

Let us perform monotonic correlation tests for each pair of variables:
```{r correlation-tests-all}
cor.test(snore$age, snore$height, method = "spearman", exact = FALSE)
cor.test(snore$age, snore$weight, method = "spearman", exact = FALSE)
cor.test(snore$age, snore$bmi, method = "spearman", exact = FALSE)
cor.test(snore$age, snore$alcool, method = "spearman", exact = FALSE)
cor.test(snore$height, snore$weight, method = "spearman", exact = FALSE)
cor.test(snore$height, snore$bmi, method = "spearman", exact = FALSE)
cor.test(snore$height, snore$alcool, method = "spearman", exact = FALSE)
cor.test(snore$weight, snore$bmi, method = "spearman", exact = FALSE)
cor.test(snore$weight, snore$alcool, method = "spearman", exact = FALSE)
cor.test(snore$bmi, snore$alcool, method = "spearman", exact = FALSE)
```

In conclusion, only weight, height and BMI are correlated (all p<0.01), and in a positive trend (all rho>0), which is not surprising as BMI is calculated from weight and height. Moreover, we know that these two last metrics are heavily correlated (1), which is confirmed here.\

Reference:

(1) Silverman, M.P. (2022) Exact Statistical Distribution and Correlation of Human Height and Weight: Analysis and Experimental Confirmation. Open Journal of Statistics, 12, 743-787. https://doi.org/10.4236/ojs.2022.125044
